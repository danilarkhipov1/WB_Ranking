{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef6485e5-30ff-4010-8ea6-8fe7e85cfdc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T18:15:55.834700Z",
     "iopub.status.busy": "2024-04-02T18:15:55.834013Z",
     "iopub.status.idle": "2024-04-02T18:15:57.910925Z",
     "shell.execute_reply": "2024-04-02T18:15:57.910212Z",
     "shell.execute_reply.started": "2024-04-02T18:15:55.834665Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import bootstrap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf1085b5-f3fd-4207-9373-d34794b5d159",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T18:15:57.912763Z",
     "iopub.status.busy": "2024-04-02T18:15:57.912367Z",
     "iopub.status.idle": "2024-04-02T18:16:37.972834Z",
     "shell.execute_reply": "2024-04-02T18:16:37.972221Z",
     "shell.execute_reply.started": "2024-04-02T18:15:57.912733Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pl.read_parquet(\"C:\\\\Users\\\\User\\\\Downloads\\\\train_features.parquet\")\n",
    "df_test = pl.read_parquet(\"C:\\\\Users\\\\User\\\\Downloads\\\\test_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f5691e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>query_id</th><th>report_date</th><th>target</th><th>rn</th><th>feature_1</th><th>feature_2</th><th>feature_3</th><th>feature_4</th><th>feature_5</th><th>feature_6</th><th>feature_7</th><th>feature_8</th><th>feature_9</th><th>feature_10</th><th>feature_11</th></tr><tr><td>u32</td><td>datetime[ns]</td><td>bool</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>2</td><td>2024-01-11 00:00:00</td><td>false</td><td>4</td><td>0.693</td><td>0.1</td><td>0.245833</td><td>3.376496</td><td>0.7777</td><td>0.0</td><td>2.87166</td><td>0.97</td><td>1.0</td><td>4.6</td><td>-0.196707</td></tr><tr><td>2</td><td>2024-01-11 00:00:00</td><td>false</td><td>5</td><td>0.281</td><td>0.1</td><td>0.108333</td><td>2.909165</td><td>0.7882</td><td>0.0</td><td>1.978092</td><td>0.96</td><td>0.89</td><td>4.8</td><td>0.034091</td></tr><tr><td>2</td><td>2024-01-11 00:00:00</td><td>false</td><td>8</td><td>0.319</td><td>0.183333</td><td>0.154167</td><td>1.725731</td><td>0.688</td><td>0.5</td><td>2.083885</td><td>0.95</td><td>0.9</td><td>4.7</td><td>0.286658</td></tr><tr><td>2</td><td>2024-01-11 00:00:00</td><td>false</td><td>10</td><td>0.281</td><td>0.1</td><td>0.108333</td><td>3.759698</td><td>0.7882</td><td>1.0</td><td>1.898608</td><td>0.98</td><td>0.81</td><td>4.8</td><td>0.052632</td></tr><tr><td>2</td><td>2024-01-11 00:00:00</td><td>false</td><td>12</td><td>0.281</td><td>0.1</td><td>0.108333</td><td>0.026253</td><td>0.7055</td><td>0.0</td><td>0.317658</td><td>0.96</td><td>0.85</td><td>4.7</td><td>0.172424</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 15)\n",
       "┌──────────┬──────────────┬────────┬─────┬───┬───────────┬───────────┬────────────┬────────────┐\n",
       "│ query_id ┆ report_date  ┆ target ┆ rn  ┆ … ┆ feature_8 ┆ feature_9 ┆ feature_10 ┆ feature_11 │\n",
       "│ ---      ┆ ---          ┆ ---    ┆ --- ┆   ┆ ---       ┆ ---       ┆ ---        ┆ ---        │\n",
       "│ u32      ┆ datetime[ns] ┆ bool   ┆ i64 ┆   ┆ f64       ┆ f64       ┆ f64        ┆ f64        │\n",
       "╞══════════╪══════════════╪════════╪═════╪═══╪═══════════╪═══════════╪════════════╪════════════╡\n",
       "│ 2        ┆ 2024-01-11   ┆ false  ┆ 4   ┆ … ┆ 0.97      ┆ 1.0       ┆ 4.6        ┆ -0.196707  │\n",
       "│          ┆ 00:00:00     ┆        ┆     ┆   ┆           ┆           ┆            ┆            │\n",
       "│ 2        ┆ 2024-01-11   ┆ false  ┆ 5   ┆ … ┆ 0.96      ┆ 0.89      ┆ 4.8        ┆ 0.034091   │\n",
       "│          ┆ 00:00:00     ┆        ┆     ┆   ┆           ┆           ┆            ┆            │\n",
       "│ 2        ┆ 2024-01-11   ┆ false  ┆ 8   ┆ … ┆ 0.95      ┆ 0.9       ┆ 4.7        ┆ 0.286658   │\n",
       "│          ┆ 00:00:00     ┆        ┆     ┆   ┆           ┆           ┆            ┆            │\n",
       "│ 2        ┆ 2024-01-11   ┆ false  ┆ 10  ┆ … ┆ 0.98      ┆ 0.81      ┆ 4.8        ┆ 0.052632   │\n",
       "│          ┆ 00:00:00     ┆        ┆     ┆   ┆           ┆           ┆            ┆            │\n",
       "│ 2        ┆ 2024-01-11   ┆ false  ┆ 12  ┆ … ┆ 0.96      ┆ 0.85      ┆ 4.7        ┆ 0.172424   │\n",
       "│          ┆ 00:00:00     ┆        ┆     ┆   ┆           ┆           ┆            ┆            │\n",
       "└──────────┴──────────────┴────────┴─────┴───┴───────────┴───────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054bf2a6",
   "metadata": {},
   "source": [
    "## Посмотрим на пропуски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a64fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>query_id</th><th>report_date</th><th>target</th><th>rn</th><th>feature_1</th><th>feature_2</th><th>feature_3</th><th>feature_4</th><th>feature_5</th><th>feature_6</th><th>feature_7</th><th>feature_8</th><th>feature_9</th><th>feature_10</th><th>feature_11</th></tr><tr><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>3266</td><td>0</td><td>0</td><td>7605609</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 15)\n",
       "┌──────────┬─────────────┬────────┬─────┬───┬───────────┬───────────┬────────────┬────────────┐\n",
       "│ query_id ┆ report_date ┆ target ┆ rn  ┆ … ┆ feature_8 ┆ feature_9 ┆ feature_10 ┆ feature_11 │\n",
       "│ ---      ┆ ---         ┆ ---    ┆ --- ┆   ┆ ---       ┆ ---       ┆ ---        ┆ ---        │\n",
       "│ u32      ┆ u32         ┆ u32    ┆ u32 ┆   ┆ u32       ┆ u32       ┆ u32        ┆ u32        │\n",
       "╞══════════╪═════════════╪════════╪═════╪═══╪═══════════╪═══════════╪════════════╪════════════╡\n",
       "│ 0        ┆ 0           ┆ 0      ┆ 0   ┆ … ┆ 7605609   ┆ 0         ┆ 0          ┆ 0          │\n",
       "└──────────┴─────────────┴────────┴─────┴───┴───────────┴───────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.null_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cb7ccfe-2a05-4fa7-b064-f23c475c1297",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T18:16:37.974574Z",
     "iopub.status.busy": "2024-04-02T18:16:37.973983Z",
     "iopub.status.idle": "2024-04-02T18:16:40.629714Z",
     "shell.execute_reply": "2024-04-02T18:16:40.628992Z",
     "shell.execute_reply.started": "2024-04-02T18:16:37.974543Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Преобразуем DataFrame из Polars в Pandas\n",
    "df_train_pd = df_train.to_pandas()\n",
    "df_test_pd = df_test.to_pandas()\n",
    "\n",
    "# Группируем данные по query_id с сохранением уникальных значений rn\n",
    "grouped = df_train_pd.groupby(\"query_id\", as_index=False).first()\n",
    "\n",
    "# Разделение на тренировочный и валидационный наборы\n",
    "train_query_ids, val_query_ids = train_test_split(\n",
    "    grouped[\"query_id\"], random_state=42, test_size=0.2)\n",
    "\n",
    "# Отбор тренировочных и валидационных данных\n",
    "train_df = df_train_pd[df_train_pd[\"query_id\"].isin(train_query_ids)]\n",
    "val_df = df_test_pd[df_test_pd[\"query_id\"].isin(val_query_ids)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8c4510",
   "metadata": {},
   "source": [
    "У меня не получалось нормально сгруппировать в polars поэтому сделал это в pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5050fdcc-7368-4edd-90ec-5140acf0a662",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T18:16:48.949443Z",
     "iopub.status.busy": "2024-04-02T18:16:48.948754Z",
     "iopub.status.idle": "2024-04-02T18:16:49.591838Z",
     "shell.execute_reply": "2024-04-02T18:16:49.591239Z",
     "shell.execute_reply.started": "2024-04-02T18:16:48.949402Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Преобразование DataFrame из Pandas в Polars\n",
    "train_df = pl.from_pandas(train_df)\n",
    "val_df = pl.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a6447d1-ec00-4d8d-8b17-daa8659b6f98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T18:16:52.793122Z",
     "iopub.status.busy": "2024-04-02T18:16:52.792197Z",
     "iopub.status.idle": "2024-04-02T18:16:52.802138Z",
     "shell.execute_reply": "2024-04-02T18:16:52.801241Z",
     "shell.execute_reply.started": "2024-04-02T18:16:52.793069Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_columns = [f\"feature_{i}\" for i in range(1, 12)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed4594f",
   "metadata": {},
   "source": [
    "## Предобработка признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70d0c02",
   "metadata": {},
   "source": [
    "Заполнитель нулей сделал медианными значениями, т.к очень много выбросов в данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "708ad5d3-e337-454d-875f-2c6ab1ecb311",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T18:16:54.754477Z",
     "iopub.status.busy": "2024-04-02T18:16:54.753817Z",
     "iopub.status.idle": "2024-04-02T18:16:57.120318Z",
     "shell.execute_reply": "2024-04-02T18:16:57.119623Z",
     "shell.execute_reply.started": "2024-04-02T18:16:54.754441Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MedianPLImputer:\n",
    "    def __init__(self):\n",
    "        self.feature_medians = {}\n",
    "        \n",
    "    def fit(self, x: pl.DataFrame): \n",
    "        self.feature_medians = x.median().to_dicts()[0]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x: pl.DataFrame) -> pl.DataFrame:\n",
    "        return (\n",
    "            x\n",
    "            .with_columns(*[\n",
    "                pl.col(col).fill_null(val)\n",
    "                for col, val in self.feature_medians.items()\n",
    "            ])\n",
    "        )\n",
    "    \n",
    "imputer = MedianPLImputer().fit(train_df.select(*feature_columns))\n",
    "\n",
    "train_df = imputer.transform(train_df)\n",
    "val_df = imputer.transform(val_df)\n",
    "\n",
    "df_test = imputer.transform(df_test)\n",
    "df_train = imputer.transform(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf07e0c2-e6e5-41ba-8582-fb36742cffe1",
   "metadata": {},
   "source": [
    "## Метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e23185",
   "metadata": {},
   "source": [
    "Опираться в выборе метрик буду на статью с хабра <br>\n",
    "https://habr.com/ru/companies/econtenta/articles/303458/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da0a18",
   "metadata": {},
   "source": [
    "Также возьму метрики из бейзлайна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6e633a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Эта функция считает все популярные метрики, которые нашёл. В дальнейшем подумаю какие наиболее подходящие\n",
    "from scipy.stats import bootstrap\n",
    "\n",
    "\n",
    "def ap_at_k(relevances, k=10):\n",
    "    total_relevant = sum(relevances[:k])\n",
    "    \n",
    "    if total_relevant == 0:\n",
    "        return 0\n",
    "    \n",
    "    ap_ = 0\n",
    "    for k_ in range(1, k+1):\n",
    "        ap_ += sum(relevances[:k_]) * relevances[k_ - 1] / k_\n",
    "        \n",
    "    return ap_ / total_relevant\n",
    "def dcg_at_k(relevances, k=10):\n",
    "    relevances = np.asarray(relevances)[:k]\n",
    "    discounts = np.log2(np.arange(len(relevances)) + 2)\n",
    "    return np.sum(relevances / discounts)\n",
    "\n",
    "def ndcg_at_k(relevances, k=10):\n",
    "    ideal_dcg = dcg_at_k(sorted(relevances, reverse=True), k)\n",
    "    if ideal_dcg == 0:\n",
    "        return 0\n",
    "    return dcg_at_k(relevances, k) / ideal_dcg\n",
    "\n",
    "def precision_at_k(relevances, k=10):\n",
    "    return np.mean(relevances[:k])\n",
    "\n",
    "def err_at_k(relevances, k=10):\n",
    "    p = 1.0\n",
    "    err = 0.0\n",
    "    for i in range(min(k, len(relevances))):\n",
    "        rel = relevances[i]\n",
    "        err += (2**rel - 1) / (i + 1) * p\n",
    "        p *= (1 - rel)\n",
    "    return err\n",
    "\n",
    "def p_found_at_k(relevances, k=10):\n",
    "    p = 1.0\n",
    "    p_found = 0.0\n",
    "    for i in range(min(k, len(relevances))):\n",
    "        rel = relevances[i]\n",
    "        p_found += rel * p\n",
    "        p *= (1 - rel)\n",
    "    return p_found / k\n",
    "\n",
    "def calc_metrics(df, rn_col):\n",
    "    return (\n",
    "        df\n",
    "        .sort_values(rn_col)\n",
    "        .groupby(\"query_id\")\n",
    "        .agg(\n",
    "            map_at_1=(\"target\", lambda x: ap_at_k(x.tolist(), k=1)),\n",
    "            map_at_5=(\"target\", lambda x: ap_at_k(x.tolist(), k=5)),\n",
    "            map_at_10=(\"target\", lambda x: ap_at_k(x.tolist(), k=10)),\n",
    "            recall_at_1=(\"target\", lambda x: sum(x[:1]) / sum(x)),\n",
    "            recall_at_5=(\"target\", lambda x: sum(x[:5]) / sum(x)),\n",
    "            recall_at_10=(\"target\", lambda x: sum(x[:10]) / sum(x)),\n",
    "            mrr=(\"target\", lambda x: 1 / (x.tolist().index(True) + 1)),\n",
    "            hit_at_1=(\"target\", lambda x: x[:1].sum() > 0),\n",
    "            hit_at_5=(\"target\", lambda x: x[:5].sum() > 0),\n",
    "            hit_at_10=(\"target\", lambda x: x[:10].sum() > 0),\n",
    "            precision_at_1=(\"target\", lambda x: precision_at_k(x.tolist(), k=1)),\n",
    "            precision_at_5=(\"target\", lambda x: precision_at_k(x.tolist(), k=5)),\n",
    "            precision_at_10=(\"target\", lambda x: precision_at_k(x.tolist(), k=10)),\n",
    "            avg_precision_at_1=(\"target\", lambda x: ap_at_k(x.tolist(), k=1)),\n",
    "            avg_precision_at_5=(\"target\", lambda x: ap_at_k(x.tolist(), k=5)),\n",
    "            avg_precision_at_10=(\"target\", lambda x: ap_at_k(x.tolist(), k=10)),\n",
    "            ndcg_at_1=(\"target\", lambda x: ndcg_at_k(x.tolist(), k=1)),\n",
    "            ndcg_at_5=(\"target\", lambda x: ndcg_at_k(x.tolist(), k=5)),\n",
    "            ndcg_at_10=(\"target\", lambda x: ndcg_at_k(x.tolist(), k=10)),\n",
    "            err_at_10=(\"target\", lambda x: err_at_k(x.tolist(), k=10)),\n",
    "            p_found_at_10=(\"target\", lambda x: p_found_at_k(x.tolist(), k=10))\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a8045e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "map_at_1               0.141018\n",
       "map_at_5               0.224893\n",
       "map_at_10              0.243538\n",
       "recall_at_1            0.135303\n",
       "recall_at_5            0.367538\n",
       "recall_at_10           0.512123\n",
       "mrr                    0.264451\n",
       "hit_at_1               0.141018\n",
       "hit_at_5               0.380652\n",
       "hit_at_10              0.527629\n",
       "precision_at_1         0.141018\n",
       "precision_at_5         0.077806\n",
       "precision_at_10        0.054622\n",
       "avg_precision_at_1     0.141018\n",
       "avg_precision_at_5     0.224893\n",
       "avg_precision_at_10    0.243538\n",
       "ndcg_at_1              0.141018\n",
       "ndcg_at_5              0.257128\n",
       "ndcg_at_10             0.304591\n",
       "err_at_10              0.244825\n",
       "p_found_at_10          0.052763\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_metrics(df_train.to_pandas(), \"rn\").agg(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae5b4e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "map_at_1               0.143671\n",
       "map_at_5               0.228271\n",
       "map_at_10              0.245740\n",
       "recall_at_1            0.137466\n",
       "recall_at_5            0.371391\n",
       "recall_at_10           0.510242\n",
       "mrr                    0.267064\n",
       "hit_at_1               0.143671\n",
       "hit_at_5               0.385122\n",
       "hit_at_10              0.524978\n",
       "precision_at_1         0.143671\n",
       "precision_at_5         0.078640\n",
       "precision_at_10        0.054406\n",
       "avg_precision_at_1     0.143671\n",
       "avg_precision_at_5     0.228271\n",
       "avg_precision_at_10    0.245740\n",
       "ndcg_at_1              0.143671\n",
       "ndcg_at_5              0.260352\n",
       "ndcg_at_10             0.305938\n",
       "err_at_10              0.247351\n",
       "p_found_at_10          0.052498\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_metrics(df_test.to_pandas(), \"rn\").agg(\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d23f874",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f4cf5f",
   "metadata": {},
   "source": [
    "Построил логистическую регрессию на парах (релевантный - нерелевантный отклик) с учётом позиционного смещения "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82c2f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "def batchify(sequence, batch_size, bar=False, leave=True):\n",
    "    if batch_size is None:\n",
    "        yield sequence\n",
    "    else:\n",
    "        l = len(sequence)\n",
    "\n",
    "        for ndx in tqdm(range(0, l, batch_size), disable=not bar, total=l // batch_size + 1, leave=leave):\n",
    "            yield sequence.slice(ndx, min(ndx + batch_size, l))\n",
    "\n",
    "\n",
    "def mine_pairs(df_group, random_gen, n_neg_per_pos=1):\n",
    "    positive_df = df_group.filter(pl.col(\"target\") == True)\n",
    "    n_positive_nmids = positive_df.shape[0]\n",
    "\n",
    "    negative_df = df_group.filter(pl.col(\"target\") == False)\n",
    "    n_negative = min(n_neg_per_pos * n_positive_nmids, negative_df.shape[0])\n",
    "\n",
    "    ids = random_gen.choice(range(negative_df.shape[0]), size=n_negative, replace=False)\n",
    "    negative_dfs = list(batchify(negative_df, batch_size=n_positive_nmids, bar=False))\n",
    "\n",
    "    return {\"positive\": positive_df, \"negatives\": negative_dfs, \"rn\": df_group[\"rn\"]}\n",
    "\n",
    "\n",
    "def make_substact_features(mined_dfs, feature_columns):\n",
    "    return [\n",
    "        (mined_dfs[\"positive\"].select(feature_columns + [\"rn\"]) - df.select(feature_columns + [\"rn\"])).to_numpy()\n",
    "        for df in mined_dfs[\"negatives\"]\n",
    "    ]\n",
    "\n",
    "def _f(query_id, df_group, feature_columns, random_state):\n",
    "    random_gen = np.random.default_rng(random_state)\n",
    "\n",
    "    mined_dfs = mine_pairs(df_group, random_gen, n_neg_per_pos=1)\n",
    "    features_ = make_substact_features(mined_dfs, feature_columns)\n",
    "    ids_ = [[query_id]] * sum(f.shape[0] for f in features_)\n",
    "    rn_ = [df[\"rn\"].to_numpy() for df in mined_dfs[\"negatives\"]]  \n",
    "\n",
    "    return features_, ids_, rn_  # Возвращаем также значение rn\n",
    "\n",
    "\n",
    "random_gen = np.random.default_rng(342)\n",
    "results = Parallel(n_jobs=10)(\n",
    "    delayed(_f)(query_id, df_group, feature_columns, random_gen.integers(100000, size=1)[0])\n",
    "    for query_id, df_group in tqdm(df_train.groupby(\"query_id\"), total=df_train.select(\"query_id\").n_unique()))\n",
    "\n",
    "features = list(chain.from_iterable([f for f, _, _ in results]))\n",
    "ids = list(chain.from_iterable([i for _, i, _ in results]))\n",
    "rn = list(chain.from_iterable([r for _, _, r in results]))\n",
    "\n",
    "rn_flat = [item for sublist in rn for item in sublist]  \n",
    "query_ids_flat = [query_id[0] for sublist in ids for query_id in sublist] \n",
    "features_df = pl.DataFrame({\n",
    "    f\"feature_{i+1}\": np.concatenate([f[:, i] for f in features]) for i in range(len(feature_columns))\n",
    "})\n",
    "features_df = features_df.with_columns(target=pl.Series([1] * features_df.shape[0]))\n",
    "features_df = features_df.with_columns(\n",
    "    rn=pl.Series(rn_flat, dtype=pl.Int64),  \n",
    "    query_id=pl.Series(query_ids_flat, dtype=pl.UInt32)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969439d5",
   "metadata": {},
   "source": [
    "В этом методе переписал часть функций для возможного ускорения работы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ef2d0",
   "metadata": {},
   "source": [
    "Очень долго и безуспешно пытался просто поделить на батчи и сгладить изначальные данные, но не получилось. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11b358f-c020-48b9-ac83-8ba9d6d37fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "def get_model(degree: int = 1, include_bias=False, **logreg_params):\n",
    "    return make_pipeline(\n",
    "        PolynomialFeatures(degree=degree, include_bias=include_bias),\n",
    "        LogisticRegression(\n",
    "            random_state=42, \n",
    "            max_iter=1000,\n",
    "            solver=\"liblinear\",\n",
    "            **logreg_params\n",
    "        )\n",
    "    )\n",
    "\n",
    "train_query_ids_set = set(train_query_ids.to_list())\n",
    "\n",
    "train_df = features_df.filter(pl.col(\"query_id\").apply(lambda x: x in train_query_ids_set))\n",
    "\n",
    "model = get_model(degree=1, penalty=\"l1\", C=0.1)\n",
    "model.fit(\n",
    "    train_df[feature_columns], \n",
    "    train_df[\"target\"],\n",
    "    logisticregression__sample_weight=train_df[\"rn\"]  # Использование rn в качестве весовых коэффициентов\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e83b5c1-0165-4b03-b12e-d81fe1e52345",
   "metadata": {},
   "source": [
    "В дальнейшем попробую lgbm с алгоритмом для устранения позиционного отклонения. <br>\n",
    "В статье https://arxiv.org/pdf/1809.05818.pdf боролись с подобной проблемой и модель с Pairwise Debiasing получилось добиться наилучших результатов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336a497b-2e02-4913-a832-93d06dccf0f1",
   "metadata": {},
   "source": [
    "Также попробую NN модели. В статье https://arxiv.org/abs/1810.09591 исследовали ранжирование в поиске и показали, что применение нейронок может быть очень полезно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e289f-be3e-44d4-bde5-a10b9de9f50a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70eba4a-fe30-46de-ab1e-0b9e57be6309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4103370-853f-4fa8-815b-e2fa60b077dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd25c8e-9ec2-4420-bfe4-7736926a57ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3489e436-bc12-45ae-8ebe-82f158502c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
